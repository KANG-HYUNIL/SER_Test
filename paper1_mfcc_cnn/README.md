# Paper 1: MFCC + CNN + Transformer (RAVDESS)

---

## ğŸ‡°ğŸ‡· í”„ë¡œì íŠ¸ ê°œìš” (í•œê¸€)
ì´ ì €ì¥ì†ŒëŠ” "Speech Emotion Recognition Using Mel-Frequency Cepstral Coefficients & Convolutional Neural Networks" (IDCIoT 2024) ë…¼ë¬¸ì„ RAVDESS ë°ì´í„°ì…‹ ê¸°ë°˜ 3-branch êµ¬ì¡°(CNN + CNN + Transformer)ë¡œ ì¬êµ¬í˜„/ê²€ì¦í•˜ëŠ” í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.

### ì£¼ìš” íŠ¹ì§•
- **ì…ë ¥**: MFCC íŠ¹ì§• (n_mfcc=40, time frames=282)
- **ëª¨ë¸**: 3-branch (CNN 2ê°œ + Transformer 1ê°œ) â†’ concat â†’ FC ë¶„ë¥˜ê¸°
- **ë°ì´í„°ì…‹**: RAVDESS (Ryerson Audio-Visual Database of Emotional Speech and Song)

---

## Overview (English)
This module implements a speech emotion recognition (SER) pipeline using:
- **Input**: MFCC features (n_mfcc=40, time frames=282)
- **Model**: 3-branch (2x CNN + 1x Transformer) with feature concatenation and FC classifier
- **Dataset**: RAVDESS (Ryerson Audio-Visual Database of Emotional Speech and Song)

---

## ğŸ‡°ğŸ‡· ë°ì´í„° ì¤€ë¹„ ë°©ë²•
1. **RAVDESS ë‹¤ìš´ë¡œë“œ**
	 - ê³µì‹: [Zenodo RAVDESS](https://zenodo.org/record/1188976)
	 - ëª¨ë“  ì˜¤ë””ì˜¤ íŒŒì¼ì„ `../datasets/`ì— ì••ì¶• í•´ì œ (í´ë” êµ¬ì¡° ìœ ì§€)
	 - ì˜ˆì‹œ: `../datasets/Actor_01/03-01-01-01-01-01-01.wav` ë“±
2. **ì „ì²˜ë¦¬ ì‹¤í–‰**
	 - ê° ì˜¤ë””ì˜¤ì—ì„œ MFCC(40x282) ì¶”ì¶œ, í•™ìŠµì…‹ì—ë§Œ AWGN ë…¸ì´ì¦ˆ ì¦ê°•
	 - `.pt` íŒŒì¼ë¡œ ì €ì¥, manifest CSV ìƒì„±
	 - ì‹¤í–‰:
		 ```bash
		 python scripts/prepare_data.py
		 ```

## Data Preparation (English)
1. **Download RAVDESS**
	 - Official: [Zenodo RAVDESS](https://zenodo.org/record/1188976)
	 - Unzip all audio files into `../datasets/` (default path)
	 - Folder structure should be: `../datasets/Actor_01/03-01-01-01-01-01-01.wav`, etc.
2. **Preprocess**
	 - Extracts MFCC (40x282) for each audio, applies AWGN noise augmentation (train set only)
	 - Saves features as `.pt` files and creates a manifest CSV
	 - Run:
		 ```bash
		 python scripts/prepare_data.py
		 ```

---

## ğŸ‡°ğŸ‡· ëª¨ë¸ êµ¬ì¡°
- **Branch 1 (CNN1)**: 3ë‹¨ Conv2d + BatchNorm + ReLU + MaxPool, 512ì°¨ì› ë²¡í„° ì¶œë ¥
- **Branch 2 (CNN2)**: CNN1ê³¼ ë™ì¼, ê°€ì¤‘ì¹˜ ë³„ë„
- **Branch 3 (Transformer)**: MaxPool2d(1x4) â†’ Linear(70â†’512) â†’ 4-layer TransformerEncoder(4 heads) â†’ Linear(512â†’1) per token â†’ 40ì°¨ì› ë²¡í„°
- **Fusion**: [CNN1, CNN2, Transformer] concat (512+512+40=1064)
- **Classifier**: Linear(1064â†’8), LogSoftmax

## Model Architecture (English)
- **Branch 1 (CNN1)**: 3-layer Conv2d + BatchNorm + ReLU + MaxPool, output 512-dim vector
- **Branch 2 (CNN2)**: Same as CNN1, separate weights
- **Branch 3 (Transformer)**: MaxPool2d (1x4) â†’ Linear(70â†’512) â†’ 4-layer TransformerEncoder (4 heads) â†’ Linear(512â†’1) per token â†’ output 40-dim vector
- **Fusion**: Concatenate [CNN1, CNN2, Transformer] (512+512+40=1064)
- **Classifier**: Linear(1064â†’8), LogSoftmax

---

## ğŸ‡°ğŸ‡· í•™ìŠµ ë°©ë²•
- manifest.csvë¥¼ ê¸°ë°˜ìœ¼ë¡œ split/label ì •ë³´ ì‚¬ìš©
- ì‹¤í—˜ ì¶”ì : MLflow, ì„¤ì • ê´€ë¦¬: Hydra
- validation accuracy ê¸°ì¤€ early stopping
- ì‹¤í–‰:
	```bash
	python scripts/train.py
	```

## Training (English)
- Uses manifest.csv for split/label info
- MLflow for experiment tracking, Hydra for config
- Early stopping on validation accuracy
- Run:
	```bash
	python scripts/train.py
	```

---

## ğŸ‡°ğŸ‡· í‰ê°€ ë°©ë²•
- best checkpoint ë¡œë“œ, testì…‹ ì¶”ë¡ 
- accuracy, UAR, í´ë˜ìŠ¤ë³„ ì •í™•ë„, confusion matrix ì €ì¥
- ì‹¤í–‰:
	```bash
	python scripts/eval.py
	```

## Evaluation (English)
- Loads best checkpoint, runs inference on test set
- Saves metrics (accuracy, UAR), per-class accuracy, confusion matrix
- Run:
	```bash
	python scripts/eval.py
	```

---

## ğŸ‡°ğŸ‡· ì„¤ì • ë° ì¬í˜„ì„±
- ëª¨ë“  íŒŒë¼ë¯¸í„°ëŠ” `configs/default.yaml`ì—ì„œ ê´€ë¦¬
- random seed ê³ ì •, outputs/ì— ëª¨ë“  ê²°ê³¼ ì €ì¥

## Configuration & Reproducibility (English)
- All parameters (data, augmentation, model, training) are in `configs/default.yaml`
- Set random seed in config for deterministic split
- All outputs (metrics, plots, checkpoints) saved in `outputs/`

---

## References
- [RAVDESS dataset](https://zenodo.org/record/1188976)
- Paper: IDCIoT 2024, pp. 1595â€“1602
